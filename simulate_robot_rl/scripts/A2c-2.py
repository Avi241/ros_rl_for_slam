#!/usr/bin/env python3

# -*- coding: utf-8 -*-
"""cartpole_a2c_online.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Thn8vwN_2dhgkO7caSUH3uhJkXc4rZZK
"""

import numpy as np
import torch
import rospy
import gym
from torch import nn
import matplotlib.pyplot as plt
import robot_rl_env
from std_msgs.msg import String
episode_pub = rospy.Publisher('episode_done', String, queue_size=10)
step_pub = rospy.Publisher('step_done', String, queue_size=10)

# helper function to convert numpy arrays to tensors
def t(x): return torch.from_numpy(x).float()

# Actor module, categorical actions only
class Actor(nn.Module):
    def __init__(self, state_dim, n_actions):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(state_dim, 512),
            nn.Tanh(),
            nn.Linear(512, 512),
            nn.Tanh(),
            nn.Linear(512, n_actions),
            nn.Softmax()
        )
    
    def forward(self, X):
        return self.model(X)

# Critic module
class Critic(nn.Module):
    def __init__(self, state_dim):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(state_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 1)
        )
    
    def forward(self, X):
        return self.model(X)

env = gym.make("RobotEnv-v0")
num_steps = 120
max_episodes = 200
# config
state_dim = env.observation_space.shape[0]
n_actions = env.action_space.n
actor = Actor(state_dim, n_actions)
critic = Critic(state_dim)
adam_actor = torch.optim.Adam(actor.parameters(), lr=1e-3)
adam_critic = torch.optim.Adam(critic.parameters(), lr=1e-3)
gamma = 0.99

episode_rewards = []
map_completeness = []

for i in range(max_episodes):
    done = False
    total_reward = 0
    state = env.reset()


    for j in range(num_steps):
        probs = actor(t(state))
        dist = torch.distributions.Categorical(probs=probs)
        action = dist.sample()
        
        next_state, reward, done, info = env.step(action.detach().data.numpy())
        advantage = reward + (1-done)*gamma*critic(t(next_state)) - critic(t(state))
        
        total_reward += reward
        state = next_state

        critic_loss = advantage.pow(2).mean()
        adam_critic.zero_grad()
        critic_loss.backward()
        adam_critic.step()

        actor_loss = -dist.log_prob(action)*advantage.detach()
        adam_actor.zero_grad()
        actor_loss.backward()
        adam_actor.step()
        step_pub.publish("step={0}".format(j))


        if done:
            break
    
    if i % 100 == 0:
        torch.save(actor.state_dict(), "models/actor_world_{0}_episode_{1}".format(1,i))
        torch.save(critic.state_dict(), "models/critic_world_{0}_episode_{1}".format(1,i))
        np.save("map", map_completeness)
        np.save("reward", episode_rewards)

    map_completeness.append(next_state[13])       
    episode_rewards.append(total_reward)
    episode_pub.publish("episode={0}, map_com={1} ,reward={2} ".format(i,next_state[13],total_reward))

np.save("map", map_completeness)
np.save("reward", episode_rewards)

plt.scatter(np.arange(len(episode_rewards)), episode_rewards, s=2)
plt.title("Total reward per episode (online)")
plt.ylabel("reward")
plt.xlabel("episode")
plt.show()
